{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 02\n",
    "**Kernel Methods in Machine Learning (CS-E4830)**\n",
    "\n",
    "**Release date**: 14th of February, 2019\n",
    "\n",
    "**Submission date**: 28th of February, 2019 @4PM (no late submission allowed)\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Tasks:</b>\n",
    "    \n",
    "1. [Implementing the Kernel Ridge Regression](#task_1) (**1 Point**)\n",
    "2. [Kernel pre-processing](#task_2)\n",
    " 1. [Centering](#task_2a) (**2 Points**)\n",
    " 2. [Normalization](#task_2b) (**1 Point**)\n",
    "3. [Prediction on Fingerprints](#task_3) \n",
    " 1. [Hyper-parameter optimization](#task_3a) (**1 Point**)\n",
    " 2. [Inspecting the predictions](#task_3b)\n",
    "\n",
    "</div> \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Bonus Task:</b>\n",
    "\n",
    "4. [Sklearn Pipelines](#task_4) (**1 Bonus-point**)\n",
    "    \n",
    "Implement Task 3 using sklearn pipelines. You only need to consider the kernel centering as possible pre-processing step.\n",
    "</div>\n",
    "\n",
    "**Version**: 1.0\n",
    "\n",
    "**Version history**:\n",
    "\n",
    "- 1.0: Initial version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    Please add you student number and email address to the notebook into the corresponding cell.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMAIL: firstname.lastname@aalto.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDENT_NUMBER: 000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required python packages\n",
    "All tasks in this exercise can be solved by using only function and packages imported below. Please **do not** use any other imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "33246b5a4bcf6f24640d8add34bbd632",
     "grade": false,
     "grade_id": "cell-64068a7861b1229b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as it\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, TransformerMixin, clone\n",
    "from sklearn.metrics.pairwise import rbf_kernel as rbf_kernel_sk\n",
    "from sklearn.metrics.pairwise import linear_kernel as linear_kernel_sk\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, ParameterGrid\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper around the sklearn `rbf_kernel` function. In the lecture, we use the formulation of the Gaussian kernel which includes $\\sigma$ as parameter. However, sklearn uses a slighly different implementation, which is essentially just a re-parametrisation. We wrap the sklearn function, to be aligned with the lecture formulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel_wrapper(X, Y=None, sigma=None):\n",
    "    \"\"\"\n",
    "    Wrapper around the sklearn rbf-kernel function. It converts between the\n",
    "    gamma parametrization (sklearn) and the sigma parametrization (lecture).\n",
    "    \"\"\"\n",
    "    if sigma is None:\n",
    "        sigma = np.sqrt(X.shape[1] / 2.)\n",
    "\n",
    "    return rbf_kernel_sk(X, Y, gamma=(1. / (2. * (sigma**2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Kernel Ridge Regression (1 Point) <a id='task_1'></a>\n",
    "The Kernel Ridge Regression prediction model can be written as:\n",
    "\n",
    "$$\n",
    "    g(\\mathbf{x})=\\sum_{i=1}^{n_{train}}\\alpha_i \\kappa(\\mathbf{x},\\mathbf{x}_i)=\\mathbf{k}(\\mathbf{x})\\boldsymbol{\\alpha}\n",
    "$$\n",
    "with:\n",
    "- $g:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}$ being the **prediction function**\n",
    "- $\\mathbf{k}(\\mathbf{x})=[\\kappa(\\mathbf{x},\\mathbf{x}_1),\\ldots,\\kappa(\\mathbf{x},\\mathbf{x}_{n_{train}})]\\in\\mathbb{R}^{1\\times n_{train}}$ being a row-vector containing all **similarities** between the new example $\\mathbf{x}$ and the training examples $\\mathbf{x}_i$\n",
    "- $\\boldsymbol{\\alpha}\\in\\mathbb{R}^{n_{train}}$ column-vector containing the **dual variables**\n",
    "\n",
    "Below you find the class-template for the Kernel Ridge Regression. It's functionality is split into three parts:\n",
    "\n",
    "### 1. Intialization of Regressor Object using **__init__()**\n",
    "A Kernel Ridge Regression instance can be created using its constructor, e.g.:\n",
    "```python\n",
    "# using a kernel function\n",
    "est = KernelRidgeRegression(kernel=\"gaussian\")\n",
    "\n",
    "# or using precomputed kernel matrices\n",
    "est = KernelRidgeRegression(kernel=\"precomputed\")\n",
    "```\n",
    "\n",
    "### 2. Model Training using **fit()** \n",
    "This function takes as input:\n",
    "\n",
    "- The features of the training examples $\\mathbf{X}_{train}$ or a precomputed training kernel matrix $\\mathbf{K}_{train}\\in\\mathbb{R}^{n_{train} \\times n_{train}}$\n",
    "- Their corresponding regression labels $\\mathbf{y}_{train}\\in\\mathbb{R}^{n_{train}}$.\n",
    "\n",
    "Using that, it estimates the dual variables $\\alpha_i$'s. If needed, the kernel values between the training examples, i.e. $\\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$ are calculated during the fitting process.\n",
    "```python\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "# or using precomputed kernel matrices\n",
    "KX_train = my_kernel_function(X_train)\n",
    "est.fit(KX_train, y_train)\n",
    "```\n",
    "\n",
    "### 3. Prediction for new Examples using **predict()** \n",
    "When the model parameters are fitted, than we can make predictions for a new example $\\mathbf{x}$ using the function $g(\\mathbf{x})$. \n",
    "```python\n",
    "y_test_pred = est.predict(X_test)\n",
    "\n",
    "# or using precomputed kernel matrices\n",
    "KX_test_train = my_kernel_function(X_test, X_train)\n",
    "y_test_pred = est.predict(KX_test_train, y_train)\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of <code>fit()</code> and <code>predict()</code>. Make use of the formula provided in the lecture and above. Try to avoid the explicit calculation of the matrix-inverse, but use the <code>np.linalg.solve</code> function. Do <b>not</b> use loops for the implementation, but NumPy matrix operations.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hint:</b> An example how to circumvent the direct calculation of the matrix-inverse, can be found in the <a href=\"https://mycourses.aalto.fi/mod/forum/discuss.php?d=127103\">Python tutorial</a> of this course.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b16414961246d7d8f17d79e1feccec3",
     "grade": false,
     "grade_id": "KRR",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KernelRidgeRegression(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, kernel=\"gaussian\", beta=1.0, sigma=None):\n",
    "        \"\"\"\n",
    "        Kernel Ridge Regression (KRR)\n",
    "\n",
    "        :param kernel: string, specifying which kernel to use. Can be 'gaussian', 'linear' or 'precomputed'.\n",
    "        :param beta: scalar, regularization parameter\n",
    "        :param gamma: scalar, gaussian-kernel parameter\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.X_train = None\n",
    "        self.alphas = None\n",
    "\n",
    "        # Set up kernel function\n",
    "        self.kernel = kernel\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit a KRR model using training data.\n",
    "\n",
    "        :param X_train: array-like, shape=(n_samples, n_features), feature-matrix\n",
    "            OR X_train: array-like, shape=(n_samples, n_samples), precomputed kernel matrix\n",
    "        :param y_train: array-like, shape=(n_samples,) or (n_samples, 1), label vector\n",
    "        \"\"\"\n",
    "        # Make label vector to column-vector\n",
    "        if len(y_train.shape) == 1:\n",
    "            y_train = y_train[:, np.newaxis]\n",
    "            \n",
    "        # Calculate training kernel\n",
    "        if self.kernel != \"precomputed\":\n",
    "            self.X_train = X_train\n",
    "            KX_train = self._get_kernel(self.X_train)\n",
    "        else:\n",
    "            KX_train = X_train\n",
    "            \n",
    "        # Calculate the identity matrix scaled by the regularization parameter: (beta * n_samples) * I\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Solve: alphas' = y' * inv(K + beta * n_samples * I), shape = (n_samples, 1)\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict using fitted KRR model for new data.\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_test, n_features), feature-matrix of new data.\n",
    "            OR X: array-like, shape=(n_samples_test, n_samples_train), test-training kernel matrix.\n",
    "        :return: array-like, shape=(n_samples_test,), predictions for all data points\n",
    "        \"\"\"\n",
    "        if self.alphas is None:\n",
    "            raise RuntimeError(\"Call fit-function first.\")\n",
    "            \n",
    "        if self.kernel != \"precomputed\":\n",
    "            K_test_train = self._get_kernel(X, self.X_train)\n",
    "        else:\n",
    "            K_test_train = X\n",
    "            if K_test_train.shape[1] != self.alphas.shape[0]:\n",
    "                raise RuntimeError(\"Number of training examples does not match the shape of the \"\n",
    "                                   \"provided Test-Train-Kernel matrix.\")\n",
    "\n",
    "        # Calculate the value of the prediction function for each test example\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "                \n",
    "        return g_X.flatten()\n",
    "\n",
    "    def _get_kernel(self, X, Y=None):\n",
    "        \"\"\"\n",
    "        Calcualte kernel matrix using specified kernel-function and parameters.\n",
    "\n",
    "        :param X: array-like, shape=(n_samples_A, n_features), feature-matrix of set A\n",
    "        :param Y: array-like, shape=(n_samples_B, n_features), feature-matrix of set B or None, than Y = X\n",
    "        :return: array-like, shape=(n_samples_A, n_samples_B), kernel matrix\n",
    "        \"\"\"\n",
    "        if self.kernel == \"gaussian\":\n",
    "            return gaussian_kernel_wrapper(X, Y, self.sigma)\n",
    "        elif self.kernel == \"linear\":\n",
    "            return linear_kernel_sk(X, Y)\n",
    "        elif self.kernel == \"precomputed\":\n",
    "            raise RuntimeError(\"Provide precomputed kernel matrix.\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid kernel chosen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce0184460f7135bcc05203249f06d921",
     "grade": true,
     "grade_id": "KRR_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge as sk_KernelRidgeRegression\n",
    "\n",
    "__X, __y = make_regression(n_samples=500, random_state=319)\n",
    "\n",
    "# Split to train and test\n",
    "__X_train, __X_test, __y_train, __y_test = train_test_split(__X, __y, random_state=731)\n",
    "\n",
    "# Set up estimators\n",
    "__gamma = 0.15\n",
    "__KRR = KernelRidgeRegression(beta=0.5 / __X_train.shape[0], kernel=\"gaussian\", sigma=np.sqrt(1. / (2 * __gamma)))\n",
    "__KRR_ref = sk_KernelRidgeRegression(alpha=0.5, kernel=\"rbf\", gamma=__gamma)\n",
    "\n",
    "# Fit models\n",
    "__KRR.fit(__X_train, __y_train)\n",
    "__KRR_ref.fit(__X_train, __y_train)\n",
    "np.testing.assert_equal(__KRR.alphas.shape, (__X_train.shape[0], 1))\n",
    "\n",
    "# Predict\n",
    "__y_pred = __KRR.predict(__X_test)\n",
    "__y_pred_ref = __KRR_ref.predict(__X_test)\n",
    "np.testing.assert_allclose(__y_pred, __y_pred_ref)\n",
    "\n",
    "# MSE\n",
    "np.testing.assert_allclose(mean_squared_error(__y_test, __y_pred), \n",
    "                           mean_squared_error(__y_test, __y_pred_ref))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visually inspect your KRR implementation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(320)\n",
    "\n",
    "# Generate noise cosine curve\n",
    "X = np.arange(-0.5, 5.5, 0.05)[:, np.newaxis]\n",
    "y = np.cos(2*X) + rng.randn(X.shape[0], 1) * 0.2\n",
    "\n",
    "# Split to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=767)\n",
    "\n",
    "sigmas = [0.05, 0.5, 4.]\n",
    "betas = np.array([0.01, 1, 10]) / len(X_train)\n",
    "\n",
    "fig, axr = plt.subplots(len(sigmas), len(betas), \n",
    "                        figsize=(17, 13), sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "for idx, (s, b) in enumerate(it.product(sigmas, betas)):\n",
    "    krr = KernelRidgeRegression(beta=b, sigma=s, kernel=\"gaussian\")\n",
    "    krr.fit(X_train, y_train)\n",
    "    y_pred = krr.predict(X)\n",
    "    \n",
    "    # Calculate prediction score: sklear default for regression is R^2\n",
    "    score_train = krr.score(X_train, y_train)\n",
    "    score_test = krr.score(X_test, y_test)\n",
    "    \n",
    "    i, j = np.unravel_index(idx, dims=axr.shape)\n",
    "    axr[i, j].plot(X, y_pred, label=\"Model\", c=\"black\", alpha=0.9)\n",
    "    axr[i, j].scatter(X_train, y_train, label=\"Train\", alpha=0.75)\n",
    "    axr[i, j].scatter(X_test, y_test, label=\"Test\", alpha=0.75, c=\"red\",\n",
    "                      marker=\"s\", edgecolors=\"black\")\n",
    "    axr[i, j].set_title(\"Beta=%.4f, Sigma=%.2f \\n R**2: train=%.3f, test=%.3f\" % \n",
    "                        (b, s, score_train, score_test))\n",
    "    \n",
    "    if i == (len(sigmas) - 1):\n",
    "        axr[i, j].set_xlabel(\"x\")\n",
    "    if j == 0:\n",
    "        axr[i, j].set_ylabel(\"cos(2 * x)\")\n",
    "\n",
    "_ = axr[0, 0].legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel pre-processing <a id='task_2'></a>\n",
    "\n",
    "In this task you will implement two kernel (matrix) pre-processing steps. The first one is *kernel centering* and the second one *kernel normalization*. In practice this steps might lead to better regression models and you usually should try at least to use centered kernels.\n",
    "\n",
    "### A. Kernel centering (2 Points) <a id='task_2a'></a>\n",
    "In the Pen & Paper exercise you showed, that the kernel centering (centering of the underlying feature vectors) can be done implicitly by transforming the kernel values. That means, you can calculate the simimlarity of the centered kernel between two samples $\\kappa_c(\\mathbf{x}_i, \\mathbf{x}_j)$ using:\n",
    "\n",
    "$$\n",
    "    \\kappa_c(\\mathbf{x}_i, \\mathbf{x}_j)=\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) - \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_i, \\mathbf{x}_p) - \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_p, \\mathbf{x}_j)+\\frac{1}{n_{train}^2}\\sum_{p\\in\\mathcal{I}_{train}}\\sum_{q\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_p, \\mathbf{x}_q)\n",
    "$$\n",
    "\n",
    "Here we assume that $i\\in\\mathcal{I}_A$ and $j\\in\\mathcal{I}_{B}$ are samples indices from two samples sets $A$ and $B$. The indices of the training examples are denoted with $\\mathcal{I}_{train}$.\n",
    "\n",
    "In practice we apply the centering to three matrices:\n",
    "\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<table class=\"tg\" style=\"width: 80%\">\n",
    "  <tr>\n",
    "    <th class=\"tg-0pky\">Kernel Matrix</th>\n",
    "    <th class=\"tg-0pky\">Notation</th>\n",
    "    <th class=\"tg-0pky\">$\\mathcal{I}_A$</th>\n",
    "    <th class=\"tg-0pky\">$\\mathcal{I}_B$</th>\n",
    "    <th class=\"tg-0pky\">Note</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Training kernel matrix</td>\n",
    "    <td class=\"tg-0lax\">$\\mathbf{K}_{train}\\in\\mathbb{R}^{n_{train}\\times n_{train}}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{train}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{train}$</td>\n",
    "    <td class=\"tg-0lax\">Used for fitting.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Test-training kernel matrix</td>\n",
    "    <td class=\"tg-0lax\">$\\mathbf{K}_{test,train}\\in\\mathbb{R}^{n_{test}\\times n_{train}}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{test}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{train}$</td>\n",
    "    <td class=\"tg-0lax\">Used during prediction.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">Test-test kernel matrix</td>\n",
    "    <td class=\"tg-0lax\">$\\mathbf{K}_{test,test}\\in\\mathbb{R}^{n_{test}\\times n_{test}}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{test}$</td>\n",
    "    <td class=\"tg-0lax\">$\\mathcal{I}_{test}$</td>\n",
    "    <td class=\"tg-0lax\">Needed for kernel normalizaton only.</td>\n",
    "  </tr> \n",
    "</table>\n",
    "\n",
    "#### Implementing a kernel centering class\n",
    "We can implement the kernel centering as [sklearn transformer class](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin). It requires us to implement a ```__init__()```, ```fit()``` and ```transform()``` function. \n",
    "\n",
    "##### 1. Intialization of transformer object using '__init__()'\n",
    "We can get a KernelCentering instance using:\n",
    "```python\n",
    "centerer = KernelCentering()\n",
    "```\n",
    "##### 2. Fitting the centering statistics based on the training set using 'fit()'\n",
    "We fit the necessary centering statistics using a training kernel matrix:\n",
    "```python\n",
    "centerer.fit(KX_train)\n",
    "```\n",
    "For that we calculate a the column-wise kernel value averages of the training kernel matrix $\\mathbf{K}_{train}$ and store them in a vector $\\bar{\\mathbf{k}}_{train}\\in\\mathbb{R}^{1\\times n_{train}}$:\n",
    "\n",
    "$$\n",
    "    [\\bar{\\mathbf{k}}_{train}]_i = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_i, \\mathbf{x}_p) = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}[\\mathbf{K}_{train}]_{ip} \\quad\\forall_{i \\in \\mathcal{I}_{train}}.\n",
    "$$\n",
    "\n",
    "We furthermore need to calculate the mean value of *all* kernel values $\\mu_{train}\\in\\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "    \\mu_{train} = \\frac{1}{n_{train}^2}\\sum_{p\\in\\mathcal{I}_{train}}\\sum_{q\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_p, \\mathbf{x}_q) =\\sum_{p\\in\\mathcal{I}_{train}}\\sum_{q\\in\\mathcal{I}_{train}}[\\mathbf{K}_{train}]_{pq}\n",
    "$$\n",
    "\n",
    "##### 3. Center train and test-train kernel matrix using 'transform()'\n",
    "We can transform center a given kernel matrix:\n",
    "```python\n",
    "# Training kernel matrix\n",
    "KX_train_c = centerer.transform(KX_train)  # I_A = I_train\n",
    "\n",
    "# Test-Training kernel matrix\n",
    "KX_test_train_c = centerer.transform(KX_test_train)  # I_A = I_test\n",
    "```\n",
    "\n",
    "For that we need to calculate the average kernel value between each example of $\\mathcal{I}_A$ and the training set $\\mathcal{I}_{train}$. We store this values in a column-vector $\\bar{\\mathbf{k}}_{A,train}\\in\\mathbb{R}^{n_{A}}$: \n",
    "\n",
    "$$\n",
    "    [\\bar{\\mathbf{k}}_{A,train}]_i = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_i, \\mathbf{x}_p) = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}[\\mathbf{K}_{A,train}]_{ip} \\quad\\forall_{i \\in \\mathcal{I}_{A}}.\n",
    "$$\n",
    "\n",
    "##### 4. Center test-test kernel matrix using 'transform_k_test()'\n",
    "To center the test-test kernel matirx, we $\\mathbf{K}_{A,A}$ and $\\mathbf{K}_{A,train}$:\n",
    "```python\n",
    "centerer.transform_k_test(KX_test, KX_test_train)\n",
    "```\n",
    "\n",
    "For that we (again) need to calculate the average kernel value between each example of $\\mathcal{I}_A$ and the training set $\\mathcal{I}_{train}$. We store this values in a column-vector $\\bar{\\mathbf{k}}_{A,train}\\in\\mathbb{R}^{n_{A}}$: \n",
    "\n",
    "$$\n",
    "    [\\bar{\\mathbf{k}}_{A,train}]_i = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}\\kappa(\\mathbf{x}_i, \\mathbf{x}_p) = \\frac{1}{n_{train}}\\sum_{p\\in\\mathcal{I}_{train}}[\\mathbf{K}_{A,train}]_{ip} \\quad\\forall_{i \\in \\mathcal{I}_{A}}.\n",
    "$$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of <code>fit()</code>, <code>transform()</code> and <code>transform_K_test()</code>. Make use of the formulas provided above and the Pen & Paper exercise. Do <b>not</b> use loops for the implementation, but NumPy matrix operations.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hints:</b>\n",
    "    \n",
    "- You can calculate the mean (average) value along different axes of a matrix using <code>np.mean</code>.\n",
    "- Take a look on the NumPy [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html). What is the result of the following instructions:\n",
    "    \n",
    "```python\n",
    "M = np.arange(12).reshape((4, 3))\n",
    "print(M)\n",
    "\n",
    "a = np.array([[1, 2, 3]])  # shape = (1, 3)\n",
    "print(a)\n",
    "    \n",
    "print(M - a)  # ???\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47ae3d0beb421c73cada609b526bc2fe",
     "grade": false,
     "grade_id": "kernel_centering",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class KernelCenterer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Centering statistics\n",
    "        self.k_bar_train = None\n",
    "        self.mu_train = None\n",
    "\n",
    "    def fit(self, K, y=None, **fit_params):\n",
    "        \"\"\"\n",
    "        Extract the data statistics needed for the kernel centering.\n",
    "\n",
    "        :param K: array-like, shape=(n_samples_train, n_samples_train), kernel matrix\n",
    "        :param y: array-like, shape=(n_samples,), target values. Will be ignored. \n",
    "        :return: self, returns an instance of it self.\n",
    "        \"\"\"\n",
    "        # Calculate the statistics from the training set:\n",
    "        # - k_bar_train, shape = (1, n_train)\n",
    "        # - mu_train, scalar\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return self  # allows to use the fit_transform method inhered from TransformerMixin\n",
    "\n",
    "    def transform(self, K, y=None, copy=True, K_test=False):\n",
    "        \"\"\"\n",
    "        Apply centering to a kernel matrix.\n",
    "\n",
    "        :param K: array-like, shape=(n_samples_A, n_samples_train)\n",
    "        :param y: array-like, shape=(n_samples_A,), target values. Will be ignored!\n",
    "        :param copy: boolean, indicating whether the input kernel matrix should be copied before so that the\n",
    "            centering does not effect the original matrix.\n",
    "        :return: array-like, shape=(n_samples_A, n_samples_train) centered kernel matrix.\n",
    "        \"\"\"\n",
    "        # Copy input array, if copy=True, i.e. original data is not modified.\n",
    "        if copy:\n",
    "            K = np.array(K)\n",
    "\n",
    "        # Calculate: k_bar_A_train, shape = (n_test, 1) by averaging along the rows\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert(k_bar_A_train.shape == (K.shape[0], 1))\n",
    "        \n",
    "        \n",
    "        # Center kernel matrix\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return K\n",
    "    \n",
    "    def transform_K_test(self, K, K_test_train, copy=True):\n",
    "        \"\"\"\n",
    "        Apply centering the test-test kernel matrix. This case needs to be handled separetly.\n",
    "        \n",
    "        :param K: array-like, shape=(n_samples_A, n_samples_A), test-test kernel matrix\n",
    "        :param K_test_train: array-like, shape=(n_samples_A, n_samples_train)\n",
    "        :param copy: boolean, indicating whether the input kernel matrix should be copied before so that the\n",
    "            centering does not effect the original matrix.\n",
    "        :return: array-like, shape=(n_samples_A, n_samples_A) centered kernel matrix\n",
    "        \"\"\"\n",
    "        # Copy input array, if copy=True, i.e. original data is not modified.\n",
    "        if copy:\n",
    "            K = np.array(K)\n",
    "            \n",
    "        # Calculate: k_bar_A_train, shape = (n_test, 1) by averaging along the rows of K_test_train\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert(k_bar_A_train.shape == (K.shape[0], 1))\n",
    "        \n",
    "        # Center test-test kernel matrix K\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return K\n",
    "\n",
    "    @property\n",
    "    def _pairwise(self):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37213c99fbad7a0e7cce1a156377ecc4",
     "grade": true,
     "grade_id": "kernel_centering_tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "__rng = np.random.RandomState(890)\n",
    "__X = __rng.rand(101, 31)\n",
    "__KX = rbf_kernel_sk(__X, gamma=1.2)\n",
    "\n",
    "# Fit your centerer using some training data\n",
    "__centerer = KernelCenterer().fit(__KX[:75, :75])\n",
    "np.testing.assert_equal(__centerer.k_bar_train.shape, (1, 75))\n",
    "assert(np.isscalar(__centerer.mu_train))\n",
    "\n",
    "# Transform your training data\n",
    "__KX_train_c = __centerer.transform(__KX[:75, :75])\n",
    "np.testing.assert_equal(__KX_train_c.shape, (75, 75))\n",
    "np.testing.assert_allclose(np.sum(__KX_train_c, axis=0), \n",
    "                           np.zeros((__KX_train_c.shape[0],)), atol=1e-12)\n",
    "np.testing.assert_allclose(np.sum(__KX_train_c, axis=1),\n",
    "                           np.zeros((__KX_train_c.shape[0],)), atol=1e-12)\n",
    "\n",
    "# Transform your test-training data\n",
    "__KX_test_train_c = __centerer.transform(__KX[75:, :75])\n",
    "np.testing.assert_equal(__KX_test_train_c.shape, (26, 75))\n",
    "\n",
    "# We test the test-test kernel centering by centering the features in the original space and construct a\n",
    "# linear kernel on top. In this case, centering the kernel implicitly is equivalent to centering it in the\n",
    "# feature space.\n",
    "__X_train, __X_test = train_test_split(__X, random_state=380)\n",
    "\n",
    "__feat_centerer = StandardScaler(with_std=False).fit(__X_train)\n",
    "__X_train_c = __feat_centerer.transform(__X_train) \n",
    "__X_test_c = __feat_centerer.transform(__X_test)\n",
    "\n",
    "__KX_lin_train = linear_kernel_sk(__X_train, __X_train)\n",
    "__KX_lin_test_train = linear_kernel_sk(__X_test, __X_train)\n",
    "__KX_lin_test = linear_kernel_sk(__X_test, __X_test)\n",
    "\n",
    "# build kernels based on centered feature vectors\n",
    "__KX_lin_train_c = linear_kernel_sk(__X_train_c, __X_train_c)\n",
    "np.testing.assert_allclose(np.sum(__KX_lin_train_c, axis=0), \n",
    "                           np.zeros((__KX_lin_train_c.shape[0],)), atol=1e-12)\n",
    "\n",
    "__centerer = KernelCenterer().fit(__KX_lin_train)\n",
    "__KX_lin_test_c = __centerer.transform_K_test(__KX_lin_test, __KX_lin_test_train)\n",
    "__KX_lin_test_c_ref = linear_kernel_sk(__X_test_c, __X_test_c)\n",
    "\n",
    "np.testing.assert_allclose(__KX_lin_test_c, __KX_lin_test_c_ref)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Kernel normalization (1 Point) <a id='task_2b'></a>\n",
    "\n",
    "Using a normalized kernel can be in practice lead to better models, e.g. less senstive to outliers. As for the kernel centering, if we choose the type of normalization properly, we can calculate the normalized kernel without explicit knowledge of the underlying featurespace. \n",
    "\n",
    "Given a kernel matrix $\\mathbf{K}_{A,B}$ between to sets of samples $A$ and $B$, we can normalize its entries $\\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$, with $i\\in\\mathcal{I}_A$ and $j\\in\\mathcal{I}_B$, as follows:\n",
    "\n",
    "$$\n",
    "    \\kappa(\\mathbf{x}_i, \\mathbf{x}_j) = \\frac{\\kappa(\\mathbf{x}_i, \\mathbf{x}_j)}\n",
    "        {\\sqrt{\\kappa(\\mathbf{x}_i, \\mathbf{x}_i)\\kappa(\\mathbf{x}_j, \\mathbf{x}_j)}}.\n",
    "$$\n",
    "\n",
    "The feature-vectors $\\phi(\\mathbf{x})$ are implicitly normalized, such that: \n",
    "\n",
    "$$\n",
    "    \\phi(\\mathbf{x})_n = \\frac{\\phi(\\mathbf{x})}{\\|\\phi(\\mathbf{x})\\|} = \\frac{\\phi(\\mathbf{x})}{\\sqrt{\\phi(\\mathbf{x})^T\\phi(\\mathbf{x})}}\n",
    "$$\n",
    "\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of <code>normalize_kernel()</code>. Do <b>not</b> use loops for the implementation, but NumPy matrix operations.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Hint:</b> \n",
    "    \n",
    "- Read the documentation of the NumPy function [<code>np.outer</code>](https://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html#numpy.outer). How can it help you to construct a matrix $D\\in\\mathbb{R}^{n_A \\times n_B}$ such that:\n",
    "$$\n",
    "    [D]_{ij} = \\kappa(\\mathbf{x}_i, \\mathbf{x}_i)\\kappa(\\mathbf{x}_j, \\mathbf{x}_j)\\quad?\n",
    "$$\n",
    "- Make use of element-wise division.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e491294c44408e601687507b64ac040",
     "grade": false,
     "grade_id": "kernel_normalization",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normalize_kernel(K, d_A, d_B=None, copy=True):\n",
    "    \"\"\"\n",
    "    Function implementing the kernel normalization. \n",
    "    \n",
    "    :param K: array-like, shape=(n_samples_A, n_samples_B), kernel matrix to normalize\n",
    "    :param d_A: array-like, shape=(n_samples_A,) diagonal of the kernel matrix K_AA\n",
    "    :param d_A: array-like, shape=(n_samples_B,) diagonal of the kernel matrix K_BB\n",
    "    :param copy: boolean, indicating whether the input kernel matrix should be copied before \n",
    "        normalization.\n",
    "    :return: array-like, shape=(n_samples_A, n_samples_B) normalized kernel matrix.\n",
    "    \"\"\"\n",
    "    # Copy input array, if copy=True, i.e. original data is not modified.\n",
    "    if copy:\n",
    "        K = np.array(K)\n",
    "        \n",
    "    if K.shape[0] != len(d_A):\n",
    "        raise ValueError(\"Number of elements of d_A must match the number of rows if K.\")\n",
    "        \n",
    "    if d_B is None:\n",
    "        d_B = d_A\n",
    "        \n",
    "    if K.shape[1] != len(d_B):\n",
    "        raise ValueError(\"Number of elements of d_B must match the number of columns if K.\")\n",
    "        \n",
    "    # Normalize kernel matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "580eb0feb44b1aae3b39b64b95690d83",
     "grade": true,
     "grade_id": "kernel_normalization_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "__rng = np.random.RandomState(9212)\n",
    "__X = __rng.rand(239, 22)\n",
    "__X_n = Normalizer(norm=\"l2\").fit_transform(__X)  # phi(x)_n = phi(x) / ||phi(x)||, linear kernel\n",
    "\n",
    "__KX = linear_kernel_sk(__X, __X)\n",
    "__KX_n_ref = linear_kernel_sk(__X_n, __X_n)\n",
    "\n",
    "__KX_n = normalize_kernel(__KX, np.diag(__KX), np.diag(__KX))\n",
    "assert(np.all(np.diag(__KX_n) == 1.))\n",
    "np.testing.assert_allclose(__KX_n, __KX_n_ref)\n",
    "\n",
    "__KX_test_train_n = normalize_kernel(__KX[75:, :75], np.diag(__KX[75:, 75:]), np.diag(__KX[:75, :75]))\n",
    "__KX_test_train_n_ref = linear_kernel_sk(__X_n[75:], __X_n[:75])\n",
    "np.testing.assert_allclose(__KX_test_train_n, __KX_test_train_n_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predicting molecular properties using Kernel Ridge Regression <a id='task_3'></a>\n",
    "\n",
    "In this task you will predict so called [logP-values](https://en.wikipedia.org/wiki/Partition_coefficient) for molecules. LogP is a molecular propery relevant for drug-research. To build a prediction model, you are given 300 experimental lopP values ($y_i$'s) with their corresponding molecular structure. As features ($\\mathbf{x}_i$'s) your are given so called molecular counting fingerprints. These are vectors, that represent the molecular graph as a vector indicating the number of occurances of certain substructures within the molecule. \n",
    "\n",
    "**Counting fingerprint illustration:** Here $\\mathbf{x}_i = m_i$\n",
    "\n",
    "\n",
    "![title](count_fingerprint_example.png \"Fingerprint example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(feat=\"maccs_fps.csv\", n_samples=300, idir=\"/coursedata/exercise02/\"):\n",
    "    \"\"\"\n",
    "    Read in logp data and molecule fingerprints.\n",
    "    \"\"\"\n",
    "    X = np.genfromtxt(idir + \"/\" + feat, delimiter=\",\", dtype=\"int\", comments=None)[:, 1:]\n",
    "    y = np.genfromtxt(idir + \"/logp_values.csv\", delimiter=\",\", dtype=\"float\",\n",
    "                      comments=None, usecols=(1))\n",
    "    \n",
    "    # Get a random set of samples\n",
    "    rng = np.random.RandomState(90210)\n",
    "    rnd_idx = rng.choice(X.shape[0], n_samples, replace=False)\n",
    "    \n",
    "    return X[rnd_idx], y[rnd_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Implement the hyper parameter optimization (1 point) <a id='task_3a'></a>\n",
    "Train (fit) your Kernel Ridge Regression (KRR) with Gaussian kernel on the training examples, i.e. `X_train`. To find the optimal hyper-parameter pair, i.e. KRR regularization $\\beta$ and Gaussian bandwidth parameter $\\sigma$, we search a grid of different pairs and score each one using cross-validation.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Implement the missing code parts of the <code>hyper_parameter_search_using_cv</code> function.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Note:</b>\n",
    "    \n",
    "- This function is slighly different from the one in Exercise 01, as we need to account for kernel centering and normalization. \n",
    "- The kernel matrix must be pre-calculated and pre-processed **before** it is passed to the KRR (<code>kernel=\"precomputed\"</code>). \n",
    "- When we center the input kernels, we usually also center the output ($y_i$'s). You can read the decoumentation of [<code>TransformedTargetRegressor</code>](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html#sklearn.compose.TransformedTargetRegressor) and [<code>StandardScaler</code>](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) to understand, how we can transparently do this our case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84f57e8c9f9a80b40080d9085e6ba010",
     "grade": false,
     "grade_id": "grid_search",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hyper_parameter_search_using_cv(estimator, X, y, param_grid, n_cv_folds=5, \n",
    "                                    center=False, normalize=False, center_target=True,\n",
    "                                    random_state=None):\n",
    "    \"\"\"\n",
    "    Function calculating the estimator score for a grid of hyper parameters.\n",
    "\n",
    "    :param estimator: object, subclass of RegressorMixin or ClassifierMixin and BaseEstimator\n",
    "    :param X: array-like, shape=(n_samples, n_features), feature-matrix used for training\n",
    "    :param y: array-like, shape=(n_samples,) or (n_samples, 1), label vector used for training\n",
    "    :param param_grid: dictionary,\n",
    "        keys: different parameter names\n",
    "        values: grid-values for each parameter\n",
    "    :param n_cv_folds: scalar, a KFold cross-validation is performed where the number of splits is equal the scalar.\n",
    "    :param center: boolean, indicating whether the features (x) and targets (y) should be centered\n",
    "    :param normalize: boolean, indicating whether the features (x) should be normalized\n",
    "    :param center_target: boolean, indicating whether the targets (y) should be centered. This can be used to overwrite \n",
    "        'center' for the output.\n",
    "    :param random_state: scalar, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "\n",
    "    :return: tuple = (\n",
    "            best estimator,\n",
    "            param grid as list and corresponding evaluation scores,\n",
    "            score of best parameter\n",
    "            best parameter\n",
    "            kernel centerer (or None if center == False)\n",
    "            diagonal of the (centered) training kernel matrix (or None of normalize == False)\n",
    "        )\n",
    "    \"\"\"\n",
    "    if estimator.kernel != \"precomputed\":\n",
    "        raise ValueError(\"The estimator will get a pre-computed kernel matrix.\")\n",
    "        \n",
    "    if estimator._estimator_type != \"regressor\":\n",
    "        raise ValueError(\"Parameter estimation only supported for regression classes.\")\n",
    "    \n",
    "    # Get an iterator over all parameters\n",
    "    param_grid_iter = ParameterGrid(param_grid)\n",
    "\n",
    "    # Create cross-validation object\n",
    "    cv = KFold(n_splits=n_cv_folds, random_state=random_state)\n",
    "    \n",
    "    # Store the valdidation set performance scores for all folds and parameters\n",
    "    perf_scores = np.zeros((cv.get_n_splits(), len(param_grid_iter)))  \n",
    "    \n",
    "    # If we center the input (kernel matrices) we need to center the output\n",
    "    # as well (y). In order to make this transparent, we wrap the regressor \n",
    "    # estimator into an TransformedTargetRegressor object. It takes care about\n",
    "    # the propper output value transformation.    \n",
    "    estimator = TransformedTargetRegressor(\n",
    "        regressor=estimator, \n",
    "        transformer=StandardScaler(with_mean=(center and center_target), with_std=False))\n",
    "    \n",
    "    # Iterate over the parameter grid. It contains the kernel parameters as well.\n",
    "    # We therefore first calculate the kernel matrix, and than we subset the matrix\n",
    "    # for training, validation, ...\n",
    "    for idx, param in enumerate(param_grid_iter):\n",
    "        # This implementation is currently restricted to the Gaussian kernel!\n",
    "        K = gaussian_kernel_wrapper(X, sigma=param[\"sigma\"])  \n",
    "    \n",
    "        for fold, (train_set, val_set) in enumerate(cv.split(K, y)):\n",
    "            # Separate K_train, K_val_train and K_val from K\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            assert(K_train.shape == (len(train_set), len(train_set)))\n",
    "            \n",
    "            # Separate the train and validation targets: y_train, y_val\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            # Center the kernel if needed:\n",
    "            if center:\n",
    "                # Fit the KernelCenterer using the training data\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "                # Center the different kernel matrices\n",
    "                # - Center the training matrix: K_train\n",
    "                # - Center the validation matrix: K_val (use: transform_K_test())\n",
    "                # - Center the validation-train matrix: K_val_test\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                K_train_c, K_val_c, K_val_train_c = K_train, K_val, K_val_train  # flat copies\n",
    "                \n",
    "            if normalize:\n",
    "                # Extract the diagonals of the training and validation kernel\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "                # Normalize the different kernel matrices\n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                K_train_cn, K_val_train_cn = K_train_c, K_val_train_c  # flat copies\n",
    "            \n",
    "            # Clone the estimator object to get an un-initialized object\n",
    "            est = clone(estimator)\n",
    "\n",
    "            # Set model parameters, e.g. regularization for KRR\n",
    "            est.set_params(**{\"regressor__beta\": param[\"beta\"], \n",
    "                              \"regressor__sigma\": param[\"sigma\"]})\n",
    "            \n",
    "            # Fit the model using training set\n",
    "            est.fit(K_train_cn, y_train)\n",
    "\n",
    "            # Calculate the perf. score on validation set for current fold and parameter index\n",
    "            perf_scores[fold, idx] = est.score(K_val_train_cn, y_val)\n",
    "            \n",
    "    # Find best performing hyper-parameter\n",
    "    # Average the perf. scores for each parameter across each fold\n",
    "    avg_perf_scores = np.mean(perf_scores, axis=0)\n",
    "    \n",
    "    idx_best = np.argmax(avg_perf_scores)\n",
    "    best_perf_score = avg_perf_scores[idx_best]\n",
    "    best_param = param_grid_iter[idx_best]\n",
    "\n",
    "    # Fit model using all data with the best parameters\n",
    "    est = clone(estimator)\n",
    "    est.set_params(**{\"regressor__beta\": best_param[\"beta\"], \n",
    "                      \"regressor__sigma\": best_param[\"sigma\"]})\n",
    "    \n",
    "    # Pre-calculate kernel matrix for training the final model\n",
    "    K = gaussian_kernel_wrapper(X, sigma=best_param[\"sigma\"])\n",
    "    \n",
    "    # Pre-process kernel matrix \n",
    "    centerer = None\n",
    "    if center:\n",
    "        # Fit the kernel centering and transform the training data\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        K_c = K\n",
    "    \n",
    "    d_K_c = None\n",
    "    if normalize:\n",
    "        # Normalize the kernel matrix\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    else:   \n",
    "        K_cn = K_c\n",
    "    \n",
    "    est.fit(K_cn, y)\n",
    "\n",
    "    return (est, {\"params\": list(param_grid_iter), \"scores\": avg_perf_scores}, \n",
    "            best_perf_score, best_param, centerer, d_K_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9d29c22fe0d9cb9f36f10a918a2135a2",
     "grade": true,
     "grade_id": "grid_search_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "__rng = np.random.RandomState(320)\n",
    "\n",
    "# Generate noise cosine curve\n",
    "__X = np.arange(-0.5, 5.5, 0.05)[:, np.newaxis]\n",
    "__y = np.cos(2*__X) + __rng.randn(__X.shape[0], 1) * 0.2\n",
    "\n",
    "# Split to train and test\n",
    "__X_train, __X_test, __y_train, __y_test = train_test_split(__X, __y, random_state=767)\n",
    "\n",
    "__sigmas = [0.05, 0.5, 4.]\n",
    "__betas = np.array([0.01, 1, 10]) / __X_train.shape[0]\n",
    "__param_grid = {\"sigma\": __sigmas, \"beta\": __betas}\n",
    "\n",
    "# center = False, normalize = False\n",
    "__est, __param_scores, __best_score, __best_param, __centerer, __d_K_train_c = hyper_parameter_search_using_cv(\n",
    "    KernelRidgeRegression(kernel=\"precomputed\"), __X_train, __y_train, __param_grid,\n",
    "    center=False, normalize=False, random_state=345)\n",
    "\n",
    "assert(__centerer is None)\n",
    "assert(__d_K_train_c is None)\n",
    "np.testing.assert_equal(np.round(__best_score, 4), 0.9317)\n",
    "np.testing.assert_equal((__best_param[\"beta\"] * __X_train.shape[0]), 1.0)\n",
    "np.testing.assert_equal(__best_param[\"sigma\"], 0.5)\n",
    "np.testing.assert_equal(np.round(__param_scores[\"scores\"], 3), \n",
    "                        [0.7, 0.928, 0.414, 0.589, 0.932, 0.112, 0.177, 0.735, 0.08])\n",
    "assert(isinstance(__est, TransformedTargetRegressor))\n",
    "\n",
    "# center = True, normalize = False\n",
    "__est, __param_scores, __best_score, __best_param, __centerer, __d_K_train_c = hyper_parameter_search_using_cv(\n",
    "    KernelRidgeRegression(kernel=\"precomputed\"), __X_train, __y_train, __param_grid,\n",
    "    center=True, normalize=False, random_state=345)\n",
    "\n",
    "assert(isinstance(__centerer, KernelCenterer))\n",
    "assert(__d_K_train_c is None)\n",
    "np.testing.assert_equal(np.round(__best_score, 4), 0.9316)\n",
    "np.testing.assert_equal((__best_param[\"beta\"] * __X_train.shape[0]), 1.0)\n",
    "np.testing.assert_equal(__best_param[\"sigma\"], 0.5)\n",
    "np.testing.assert_equal(np.round(__param_scores[\"scores\"], 3), \n",
    "                        [0.698, 0.928, 0.415, 0.588, 0.932, 0.109, 0.175, 0.735, 0.078])\n",
    "assert(isinstance(__est, TransformedTargetRegressor))\n",
    "\n",
    "# center = True, normalize = True\n",
    "__est, __param_scores, __best_score, __best_param, __centerer, __d_K_train_c = hyper_parameter_search_using_cv(\n",
    "    KernelRidgeRegression(kernel=\"precomputed\"), __X_train, __y_train, __param_grid,\n",
    "    center=True, normalize=True, random_state=345)\n",
    "\n",
    "assert(isinstance(__centerer, KernelCenterer))\n",
    "assert(len(__d_K_train_c) == __X_train.shape[0])\n",
    "np.testing.assert_equal(np.round(__best_score, 4), 0.9331)\n",
    "np.testing.assert_equal((__best_param[\"beta\"] * __X_train.shape[0]), 1.0)\n",
    "np.testing.assert_equal(__best_param[\"sigma\"], 0.5)\n",
    "np.testing.assert_equal(np.round(__param_scores[\"scores\"], 3), \n",
    "                        [0.698, 0.927, 0.695, 0.587, 0.933, 0.224, 0.176, 0.776, -0.016])\n",
    "assert(isinstance(__est, TransformedTargetRegressor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data and split into train and test:** We use the training set for hyper-parameter estimation using cross-validation. For the test set we calculate different perormance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = read_data()\n",
    "print(\"(#Molecules, #Features):\", X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X, y, random_state=777)\n",
    "\n",
    "# Hyper parameter grid\n",
    "param_grid = {\"beta\": np.array([0.001, 0.01, 0.1, 1.]) / X_train.shape[0],  \n",
    "              \"sigma\": [15., 20., 25., 30., 35.]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Optimize the hyper-parameters and inspect predictions <a id='task_3b'></a>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Note:</b> We need to apply the same kernel pre-processing (centering or normalization) to the test-train and test-test kernel that we use to access the 'final' performance of the model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(2, 3, figsize=(15, 10), sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "for idx, (cen, norm) in enumerate([[False, False], [True, False], [True, True]]):\n",
    "    print(\"[Center=%d, Normalize=%d]\" % (cen, norm))\n",
    "    \n",
    "    # Find best hyper-parametrs\n",
    "    est, param_scores, best_score, best_param, centerer, d_K_train_c = hyper_parameter_search_using_cv(\n",
    "        KernelRidgeRegression(kernel=\"precomputed\"), X_train, y_train, param_grid,\n",
    "        center=cen, normalize=norm, random_state=373)\n",
    "    \n",
    "    print(\"\\tBest parameter: beta=%f, sigma=%f\" % (\n",
    "        best_param[\"beta\"] * X_train.shape[0], best_param[\"sigma\"]))\n",
    "    \n",
    "    # Calculate kernels for prediction and print scores\n",
    "    K_train = gaussian_kernel_wrapper(X_train, sigma=best_param[\"sigma\"])\n",
    "    K_test = gaussian_kernel_wrapper(X_test, sigma=best_param[\"sigma\"])\n",
    "    K_test_train = gaussian_kernel_wrapper(X_test, X_train, sigma=best_param[\"sigma\"])\n",
    "    \n",
    "    # Pre-process the kernels if needed\n",
    "    if cen:\n",
    "        K_train_c = centerer.transform(K_train)\n",
    "        K_test_train_c = centerer.transform(K_test_train)\n",
    "        K_test_c = centerer.transform_K_test(K_test, K_test_train)\n",
    "    else:\n",
    "        K_train_c, K_test_c, K_test_train_c = K_train, K_test, K_test_train\n",
    "        \n",
    "    if norm:\n",
    "        K_train_cn = normalize_kernel(K_train_c, np.diag(K_train_c))\n",
    "        K_test_train_cn = normalize_kernel(K_test_train_c, np.diag(K_test_c), np.diag(K_train_c))\n",
    "    else:\n",
    "        K_train_cn, K_test_train_cn = K_train_c, K_test_train_c\n",
    "\n",
    "    # Predict values\n",
    "    y_train_pred = est.predict(K_train_cn)\n",
    "    y_test_pred = est.predict(K_test_train_cn)\n",
    "    \n",
    "    print(\"\\tR^2: train=%.3f, test=%.3f\" % (\n",
    "        est.score(K_train_cn, y_train), est.score(K_test_train_cn, y_test)))\n",
    "    print(\"\\tMSE: train=%.3f, test=%.3f\" % (\n",
    "        mean_squared_error(y_train_pred, y_train), mean_squared_error(y_test_pred, y_test)))\n",
    "    \n",
    "    \n",
    "    #################################################\n",
    "    # Plot scores for parameter pairs and predictions\n",
    "    #################################################\n",
    "    axrr[0, idx].plot([-4, 10], [-4, 10], '--')\n",
    "    axrr[0, idx].scatter(y_train, y_train_pred, edgecolors=\"black\")\n",
    "    axrr[0, idx].set_title(\"Training data \\n[Center=%d, Normalize=%d]\" % (cen, norm))\n",
    "    axrr[0, idx].set_xlabel(\"Measured logp\")\n",
    "    axrr[0, idx].set_ylabel(\"Predicted logp\")\n",
    "    axrr[1, idx].plot([-4, 10], [-4, 10], '--')\n",
    "    axrr[1, idx].scatter(y_test, y_test_pred, c=\"red\", marker=\"s\", edgecolors=\"black\")\n",
    "    axrr[1, idx].set_title(\"test data\\n[Center=%d, Normalize=%d]\" % (cen, norm))\n",
    "    axrr[1, idx].set_xlabel(\"Measured logp\")\n",
    "    axrr[1, idx].set_ylabel(\"Predicted logp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Center input kernel but not the targets:** What happens if we do not center the ouputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axrr = plt.subplots(2, 2, figsize=(15, 10), sharex=\"all\", sharey=\"all\")\n",
    "\n",
    "for idx, (cen_in, cen_out) in enumerate([[True, True], [True, False]]):\n",
    "    print(\"[Center input=%d, Center output=%d]\" % (cen_in, cen_out))\n",
    "    \n",
    "    # Find best hyper-parametrs\n",
    "    est, param_scores, best_score, best_param, centerer, D_K_train_c = hyper_parameter_search_using_cv(\n",
    "        KernelRidgeRegression(kernel=\"precomputed\"), X_train, y_train, param_grid,\n",
    "        center=cen_in, normalize=False, center_target=cen_out, random_state=373)\n",
    "    \n",
    "    print(\"\\tBest parameter: beta=%f, sigma=%f\" % (\n",
    "        best_param[\"beta\"] * X_train.shape[0], best_param[\"sigma\"]))\n",
    "    \n",
    "    # Calculate kernels for prediction and print scores\n",
    "    K_train = gaussian_kernel_wrapper(X_train, sigma=best_param[\"sigma\"])\n",
    "    K_test = gaussian_kernel_wrapper(X_test, sigma=best_param[\"sigma\"])\n",
    "    K_test_train = gaussian_kernel_wrapper(X_test, X_train, sigma=best_param[\"sigma\"])\n",
    "    \n",
    "    # Pre-process the kernels if needed\n",
    "    if cen:\n",
    "        K_train_c = centerer.transform(K_train)\n",
    "        K_test_c = centerer.transform_K_test(K_test, K_test_train)\n",
    "        K_test_train_c = centerer.transform(K_test_train)\n",
    "    else:\n",
    "        K_train_c, K_test_c, K_test_train_c = K_train, K_test, K_test_train\n",
    "\n",
    "    # Predict values\n",
    "    y_train_pred = est.predict(K_train_c)\n",
    "    y_test_pred = est.predict(K_test_train_c)\n",
    "    \n",
    "    print(\"\\tR^2: train=%.3f, test=%.3f\" % (\n",
    "        est.score(K_train_c, y_train), est.score(K_test_train_c, y_test)))\n",
    "    print(\"\\tMSE: train=%.3f, test=%.3f\" % (\n",
    "        mean_squared_error(y_train_pred, y_train), mean_squared_error(y_test_pred, y_test)))\n",
    "    \n",
    "    #################################################\n",
    "    # Plot scores for parameter pairs and predictions\n",
    "    axrr[0, idx].plot([-4, 10], [-4, 10], '--')\n",
    "    axrr[0, idx].scatter(y_train, y_train_pred, edgecolors=\"black\")\n",
    "    axrr[0, idx].set_title(\"Training data \\n[Center input=%d, Center output=%d]\" % (cen_in, cen_out))\n",
    "    axrr[0, idx].set_xlabel(\"Measured logp\")\n",
    "    axrr[0, idx].set_ylabel(\"Predicted logp\")\n",
    "    axrr[1, idx].plot([-4, 10], [-4, 10], '--')\n",
    "    axrr[1, idx].scatter(y_test, y_test_pred, c=\"red\", marker=\"s\", edgecolors=\"black\")\n",
    "    axrr[0, idx].set_title(\"Test data \\n[Center input=%d, Center output=%d]\" % (cen_in, cen_out))\n",
    "    axrr[1, idx].set_xlabel(\"Measured logp\")\n",
    "    axrr[1, idx].set_ylabel(\"Predicted logp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sklearn pipelines (1 Bonus-Point) <a id='task_4'></a>\n",
    "\n",
    "In the [previous task](#task_3a) you could see, that our hyper-parameter search function became relatively complicated. This eventually can lead to quite error-prone implementations. The sklearn package provides a nice tool to chain transformers and estimator using [pipelines](https://scikit-learn.org/stable/modules/compose.html).\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    <b>Task:</b> Earn a bonus point, by implementing the hyper-parameter search using a chain of transformers and estimators. Use the sklearn-package GridSearchCV function for your Pipeline object.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>Note / Hints:</b>\n",
    "    \n",
    "- You do <b>not</b> need integrate the kernel normalization, i.e. only centering is sufficient. Think about it: Why does kernel normalization not fits well to sklearn API? \n",
    "- Read the documentation of [<code>GridSearchCV</code>](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) and [<code>Pipeline</code>](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline)\n",
    "- Your pipline should look like: <code>GaussianKernel -> KernelCenterer -> TransformedTargetRegressor(KernelRidgeRegression, StandardScaler)</code>\n",
    "- Hyper-parameter names must be changed a bit when using pipelines.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianKernel(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Class to wrap the Gaussian kernel, so that it is an transformer estimator. \n",
    "    This allows us to use the kernel in a pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma=None):\n",
    "        self.sigma = sigma\n",
    "        self.X_train = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.X_train = X\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, copy=True):\n",
    "        return gaussian_kernel_wrapper(X, self.X_train, sigma=self.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94c9e7f8d30e11eff0adae797e383493",
     "grade": true,
     "grade_id": "bonus_pipelines",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Build your pipeline\n",
    "pipe = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Set up the parameter grid. Note: Parameter names change a bit when using a pipeline.\n",
    "param_grid = None\n",
    "betas = np.array([0.001, 0.01, 0.1, 1.]) / X_train.shape[0]\n",
    "sigmas = [15., 20., 25., 30., 35.]\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Fit a gridsearchcv object using 5-fold cv\n",
    "gscv = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best params: beta=%f, sigma=%f\" % (gscv.best_params_[\"KRR__regressor__beta\"] * X_train.shape[0], gscv.best_params_[\"gaussian__sigma\"]))\n",
    "print(\"R^2: train=%.3f, test=%.3f\" % (gscv.score(X_train, y_train), gscv.score(X_test, y_test)))\n",
    "print(\"MSE: train=%.3f, test=%.3f\" % (\n",
    "    mean_squared_error(gscv.predict(X_train), y_train),\n",
    "    mean_squared_error(gscv.predict(X_test), y_test)))\n",
    "\n",
    "fig, axrr = plt.subplots(1, 2, figsize=(15, 7))\n",
    "axrr[0].scatter(y_train, gscv.predict(X_train))\n",
    "axrr[0].plot([-4, 10], [-4, 10], '--')\n",
    "axrr[0].set_title(\"Training data\")\n",
    "axrr[0].set_xlabel(\"Measured logp\")\n",
    "axrr[0].set_ylabel(\"Predicted logp\")\n",
    "axrr[1].scatter(y_test, gscv.predict(X_test))\n",
    "axrr[1].plot([-4, 10], [-4, 10], '--')\n",
    "axrr[1].set_title(\"Test data\")\n",
    "axrr[1].set_xlabel(\"Measured logp\")\n",
    "_ = axrr[1].set_ylabel(\"Predicted logp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
