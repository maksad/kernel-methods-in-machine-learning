{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 04\n",
    "\n",
    "**Kernel Methods in Machine Learning (CS-E4830)**\n",
    "\n",
    "**Release date**: 22th of March, 2019\n",
    "\n",
    "**Submission date**: 4th of April, 2019 @4PM (no late submission allowed)\n",
    "\n",
    "\n",
    "### Canonical Correlation Analysis and Kernel Canonical Correlation Analysis\n",
    "\n",
    "In this exercise, we will implement the standard eigenvalue problem for CCA and the generalised eigenvalue problem for KCCA. The aim is to understand how CCA and KCCA are applied on two-view datasets and how the result is interpreted. All theory needed to complete these exercises can be found in the lecture material.\n",
    "\n",
    "In contrast to the previous Jupyter exercises, these tasks will be graded manually and there are no hidden tests. To see whether the codes are correct, you can compare the result with the examples given on slides 12-13 (CCA example) and 18-19 (KCCA example). The idea is to re-produce the results of those examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    Please add you student number and email address to the notebook into the corresponding cell.\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMAIL: firstname.lastname@aalto.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STUDENT_NUMBER: 000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import required python packages**\n",
    "\n",
    "All tasks in this exercise can be solved by using only function and packages imported below. Please **do not** use any other imports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy.linalg import inv\n",
    "from numpy import linalg as LA\n",
    "import pylab as pl\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CCA (2 points)**\n",
    "\n",
    "In this section, the task is to \n",
    "- complete the ***standardize_data*** function\n",
    "- complete the ***cca*** function.\n",
    "\n",
    "The functions to generate and partition the data into training and test sets will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n,p,q):\n",
    "    \"\"\"\n",
    "    Generate a simulated two-view dataset from a random uniform distribution. We focus on two-to-two relations.\n",
    "    \n",
    "    Input:  n - sample size\n",
    "            p - number of variables in the view X\n",
    "            q - number of variables in the view Y\n",
    "            \n",
    "    Output: The data matrices X and Y\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate data from random uniform distribution\n",
    "    X = np.random.uniform(-2,2,[n,p])\n",
    "    Y = np.random.uniform(-2,2,[n,q])\n",
    "   \n",
    "    # simulated relations\n",
    "    Y[:,0] = X[:,0] + X[:,1] - Y[:,1] + np.random.normal(0,0.1,n)\n",
    "    Y[:,2] = np.power(X[:,2] + X[:,3],3) - 5*Y[:,3] + np.random.normal(0,0.1,n)\n",
    "    Y[:,4] = np.sin(X[:,4] + X[:,5]) - Y[:,5] + np.random.normal(0,0.1,n)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that standardizes the variables to have a zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7abb313f8a6056c1b37feda17bc45fc4",
     "grade": true,
     "grade_id": "standardize",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize_data(X,Y):\n",
    "    \"\"\"\n",
    "    Standardize the dataset so that all variables have a zero mean and a unit variance.\n",
    "    \n",
    "    Input:  X - the first view X\n",
    "            Y - the second view Y\n",
    "            \n",
    "    Output: The standardized data matrices Xn and Yn\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return Xn, Yn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function partitions the data into training (2/3) and test (1/3) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(X,Y):\n",
    "    \"\"\"\n",
    "    Partition the observations of the sample into (2/3) training and (1/3) test sets.\n",
    "    \n",
    "    Input:  X - the first view X\n",
    "            Y - the second view Y\n",
    "            \n",
    "    Output: Xtrain, Xtest, Ytrain, Ytest - The training and test matrices for the views X and Y respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    train = int(round(2 * X.shape[0] / 3)) \n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    training_idx, test_idx = indices[:train], indices[train:]\n",
    "    Xtrain, Xtest, Ytrain, Ytest = X[training_idx,:], X[test_idx,:], Y[training_idx,:], Y[test_idx,:]  \n",
    "    \n",
    "    return Xtrain, Xtest, Ytrain, Ytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following cca function that takes as input two matrices where the rows correspond to observations and columns correspond to variables. The variable components corresponds to the number of canonical components to be extracted. The task is to \n",
    "- Fill in the blocks $\\mathbf{C}_{aa}$, $\\mathbf{C}_{ab}$, $\\mathbf{C}_{ba}$, and $\\mathbf{C}_{bb}$ of the joint covariance matrix.\n",
    "- Write the expression of the canonical correlation matrix M for which the eigenvalues are computed\n",
    "- Write the expression to compute $\\mathbf{w}_a$. You can compute the inverse using inv().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "486604b742082896c031ad8d6688a83a",
     "grade": true,
     "grade_id": "cca",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def cca(X,Y,components):\n",
    "    \"\"\"\n",
    "    This function performs CCA on the data matrices X and Y of sizes (n x p) and (n x q) respectively.\n",
    "    \n",
    "    Input:  X - the first view X\n",
    "            Y - the second view Y\n",
    "            components - scalar, the number of components to extract\n",
    "            \n",
    "    Output: cc - the canonical correlation\n",
    "            wa - the canonical coefficient vector of size (p x components) for view X\n",
    "            wb - the canonical coefficient vector of size (q x components) for view Y\n",
    "    \"\"\"\n",
    "        \n",
    "    # write the expressions of blocks of the joint covariance matrix\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the canonical correlation matrix (name it M)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the eigenvalues and eigenvectors of M\n",
    "    eigvalues, eigvectors = LA.eig(M)\n",
    "\n",
    "    # canonical correlations\n",
    "    cc = np.sqrt(-np.sort(-eigvalues))\n",
    "    inds = np.argsort(-eigvalues)\n",
    "    eigv = eigvectors[:,inds]\n",
    "       \n",
    "    # wb\n",
    "    wb = eigv[:,:components];\n",
    "    \n",
    "    # wa\n",
    "    wa = np.zeros((X.shape[1],components))\n",
    "    for ii in range(0,components):\n",
    "        # write the expression for wa\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    return cc, wa, wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function computes correlation between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation(x,y):\n",
    "    \"\"\"\n",
    "    This function computes the correlation between two vectors x and y.\n",
    "    \n",
    "    Input:  x - a vector\n",
    "            y - a vector of same size as x\n",
    "            \n",
    "    Output: rho - the correlation coefficient\n",
    "    \"\"\"\n",
    "    rho = np.inner(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))     \n",
    "    return rho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run CCA on a simulated dataset. We generate a simulated dataset that contains linear, cubic, and sinusoidal two-to-two relations. We set the sample size to 500 and 10 variables in both views. We standardize the variables to have a zero mean and a unit variance. The dataset is partitioned into training and test sets. We learn the canonical coefficient vectors $\\mathbf{w}_a$ and $\\mathbf{w}_b$ from the training data and use these on test data to evaluate whether the learnt relations are predictive for the particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data(500,10,10)   \n",
    "Xn, Yn = standardize_data(X,Y)\n",
    "Xtrain, Xtest, Ytrain, Ytest = partition_data(Xn,Yn)   \n",
    "cc, wa, wb = cca(Xtrain,Ytrain,3)\n",
    "rho_test = np.zeros(wa.shape[1])\n",
    "for ii in range(0,wa.shape[1]):\n",
    "    rho_test[ii] = compute_correlation(Xtest @ wa[:,ii], Ytest @ wb[:,ii])  \n",
    "    \n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 14))\n",
    "ax0, ax1, ax2, ax3, ax4, ax5 = axes.flatten()\n",
    "    \n",
    "ax0.plot(Xtrain @ wa[:,0],Ytrain @ wb[:,0], 'bs')\n",
    "ax0.set_ylabel('$\\mathbf{X}_b^{train} \\mathbf{w}_b$')\n",
    "ax0.set_xlabel('$\\mathbf{X}_a^{train} \\mathbf{w}_a$')\n",
    "ax0.set_title(r'$\\rho_{train}$ = %1.3f' % cc[0])\n",
    "\n",
    "ax1.plot(Xtest @ wa[:,0], Ytest @ wb[:,0], 'bs')\n",
    "ax1.set_ylabel('$\\mathbf{X}_b^{test} \\mathbf{w}_b$')\n",
    "ax1.set_xlabel('$\\mathbf{X}_a^{test} \\mathbf{w}_a$')\n",
    "ax1.set_title(r'$\\rho_{test}$ = %1.3f' % rho_test[0])\n",
    "\n",
    "ax2.plot(Xtrain @ wa[:,1],Ytrain @ wb[:,1], 'bs')\n",
    "ax2.set_ylabel('$\\mathbf{X}_b^{train} \\mathbf{w}_b$')\n",
    "ax2.set_xlabel('$\\mathbf{X}_a^{train} \\mathbf{w}_a$')\n",
    "ax2.set_title(r'$\\rho_{train}$ = %1.3f' % cc[1])\n",
    "\n",
    "ax3.plot(Xtest @ wa[:,1], Ytest @ wb[:,1], 'bs')\n",
    "ax3.set_ylabel('$\\mathbf{X}_b^{test} \\mathbf{w}_b$')\n",
    "ax3.set_xlabel('$\\mathbf{X}_a^{test} \\mathbf{w}_a$')\n",
    "ax3.set_title(r'$\\rho_{test}$ = %1.3f' % rho_test[1])\n",
    "\n",
    "ax4.plot(Xtrain @ wa[:,2],Ytrain @ wb[:,2], 'bs')\n",
    "ax4.set_ylabel('$\\mathbf{X}_b^{train} \\mathbf{w}_b$')\n",
    "ax4.set_xlabel('$\\mathbf{X}_a^{train} \\mathbf{w}_a$')\n",
    "ax4.set_title(r'$\\rho_{train}$ = %1.3f' % cc[2])\n",
    "\n",
    "ax5.plot(Xtest @ wa[:,2], Ytest @ wb[:,2], 'bs')\n",
    "ax5.set_ylabel('$\\mathbf{X}_b^{test} \\mathbf{w}_b$')\n",
    "ax5.set_xlabel('$\\mathbf{X}_a^{test} \\mathbf{w}_a$')\n",
    "ax5.set_title(r'$\\rho_{test}$ = %1.3f' % rho_test[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine()\n",
    "plt.show()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)   \n",
    "sns.heatmap(wa, ax = ax1, annot = True, linewidths=.5, cbar = False, xticklabels = ['$\\mathbf{w}_a^1$', '$\\mathbf{w}_a^2$', '$\\mathbf{w}_a^3$'], yticklabels = [i for i in range(1,11)]) \n",
    "sns.heatmap(wb, ax = ax2, annot = True, linewidths=.5, cbar = False, xticklabels = ['$\\mathbf{w}_b^1$', '$\\mathbf{w}_b^2$', '$\\mathbf{w}_b^3$'], yticklabels = [i for i in range(1,11)])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer here to the following questions:\n",
    "1. What does the value of the training correlation indicate?\n",
    "2. What does the value of the test correlation indicate?\n",
    "3. What can be deduced by observing the entries of the vectors $\\mathbf{w}_a$ and $\\mathbf{w}_b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KCCA (3 points)**\n",
    "\n",
    "In this section, we will implement the generalized eigenvalue problem for KCCA. We will compare the linear and quadratic (homogeneous polynomial kernel of degree two) KCCA. The idea is to reproduce the results of the example on lecture slides 18 and 19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate_data_trig function simulates a trigonometric multivariate relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_trig(n,p,q):\n",
    "    \"\"\"\n",
    "    This function generates a dataset that contains a non-monotonic trigonometric relation.\n",
    "    \n",
    "    Input:  n - sample size\n",
    "            p - number of variables in the view X\n",
    "            q - number of variables in the view Y\n",
    "            \n",
    "    Output: The data matrices X and Y\n",
    "    \"\"\"\n",
    "    X = np.random.uniform(-6,6,[n,p])\n",
    "    Y = np.random.uniform(-6,6,[n,q])\n",
    "   \n",
    "    X[:,1] = 3 * np.sin(X[:,2]) + np.random.normal(0,0.05,n)\n",
    "    Y[:,1] = 4 * np.cos(X[:,2]) + np.random.normal(0,0.05,n)\n",
    "    Y[:,2] = 5 * np.cos(X[:,2]) + np.random.normal(0,0.05,n)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(X[:,1] + X[:,2], Y[:,1] + Y[:,2], 'bs')\n",
    "    plt.xlabel('$\\mathbf{x}_a^1 + \\mathbf{x}_a^2$')\n",
    "    plt.ylabel('$\\mathbf{x}_b^1 + \\mathbf{x}_b^2$')\n",
    "    plt.title('Non-Monotonous Trigonometric Relation')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "       \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear kernels need to be centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_kernel(K):\n",
    "    \"\"\"\n",
    "    This centers the kernel matrix.\n",
    "    \n",
    "    Input:  K - the kernel matrix\n",
    "            \n",
    "    Output: Kc - the centered kernel matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    n1 = K.shape[0]\n",
    "    n2 = K.shape[1]\n",
    "    Kc = (np.eye(n1) - 1/n1 * np.ones(n1)) @ K @ (np.eye(n2) - 1/n2 * np.ones(n2))\n",
    "    \n",
    "    return Kc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_test_kernel(Ktest,Ktrain):\n",
    "    \"\"\"\n",
    "    This centers the test-training kernel matrix.\n",
    "    \n",
    "    Input:  Ktest - the test kernel matrix\n",
    "            Ktrain - the training kernel matrix\n",
    "            \n",
    "    Output: Kc - the centered kernel matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    n1 = Ktrain.shape[0]\n",
    "    n2 = Ktest.shape[0]\n",
    "    unit = np.ones((n1, n1)) / n1\n",
    "    unit_test = np.ones((n2,n1)) / n1\n",
    "    Kc = Ktest - unit_test @ Ktrain - Ktest @ unit + unit_test @ Ktrain @ unit\n",
    "    \n",
    "    return Kc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to complete the following function that implements the generalised eigenvalue problem for KCCA.\n",
    "- write the expressions for the regularised within-set covariance matrices (Lecture slide 16)\n",
    "- write the equation of the kernelised generalised eigenvalue problem (Lecture slide 16)\n",
    "\n",
    "*Hint. To compile the blocks of matrices for the generalised eigenvalue problem, you can use np.vstack and np.hstack.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81e644e2152076c41c00b0b70115de28",
     "grade": true,
     "grade_id": "kcca",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def kcca_gep(Kx,Ky,c1,c2,components): \n",
    "    \"\"\"\n",
    "    This function performs KCCA on the kernel matrices Kx and Ky of sizes (n x n) and (n x n) respectively.\n",
    "    \n",
    "    Input:  Kx - the kernel matrix of the first view X\n",
    "            Ky - the kernel matrix of second view Y\n",
    "            c1 - the regularisation hyperparameter on the first view\n",
    "            c2 - the regularisation hyperparameter on the second view\n",
    "            components - scalar, the number of components to extract\n",
    "            \n",
    "    Output: kcc - the kernel canonical correlation\n",
    "            alpha - the dual canonical coefficient vector of size (n x components) for first view\n",
    "            beta - the dual canonical coefficient vector of size (n x components) for second view\n",
    "    \"\"\"\n",
    "    \n",
    "    # regularize the within-set covariance matrices\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the matrix equation for the generalised eigenvalue problem\n",
    "    n = Kx.shape[0]\n",
    "    # write the expression for matrix A containing the kernel matrices Kx and Ky in the anti-diagonal\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # write the expression for matrix B containing the regularised kernel matrices in the diagonal\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the eigenvalues and eigenvectors\n",
    "    eigvalues, eigvectors = scipy.linalg.eigh(A, B, eigvals_only=False)\n",
    "    \n",
    "    # kernel canonical correlations\n",
    "    rho = np.abs(-eigvalues)**(1/2)\n",
    "    inds = np.argsort(-eigvalues)\n",
    "    eigv = eigvectors[:,inds]\n",
    "    \n",
    "    alpha = eigv[::n][0:components]\n",
    "    alpha = eigv[0:n,0:components]\n",
    "    beta = eigv[n:,0:components]\n",
    "    \n",
    "    kcc = rho[0:components]\n",
    "    \n",
    "    return kcc, alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reproduce the results of the quadratic KCCA example on lecture slides 18 and 19. First, we generate the trigonometric multivariate relation for a dataset that consists of 500 observations and 10 variables in each view. We partition this dataset into training and test sets. To compare linear and quadratic KCCA, we compute both the linear and quadratic polynomial kernels for the training and test sets. We solve the kernelized generalized eigenvalue problem for both sets of kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data_trig(500,10,10)   \n",
    "Xn, Yn = standardize_data(X,Y)\n",
    "Xtrain, Xtest, Ytrain, Ytest = partition_data(Xn,Yn)    \n",
    "\n",
    "# compute the linear kernels\n",
    "Kxtrain1 = Xtrain @ Xtrain.T\n",
    "Kytrain1 = Ytrain @ Ytrain.T   \n",
    "Kxtest1 = Xtest @ Xtrain.T\n",
    "Kytest1 = Ytest @ Ytrain.T\n",
    "\n",
    "# compute the quadratic kernels\n",
    "Kxtrain2 = center_kernel((Xtrain @ Xtrain.T)**2)\n",
    "Kytrain2 = center_kernel((Ytrain @ Ytrain.T)**2) \n",
    "Kxtest2 = center_test_kernel((Xtest @ Xtrain.T)**2,(Xtrain @ Xtrain.T)**2)\n",
    "Kytest2 = center_test_kernel((Ytest @ Ytrain.T)**2,(Ytrain @ Ytrain.T)**2)\n",
    "\n",
    "# perform linear KCCA\n",
    "kcc1, alpha1, beta1 = kcca_gep(Kxtrain1,Kytrain1,0.02,0.02,1)\n",
    "train_kcc1 = compute_correlation((Kxtrain1 @ alpha1).T, (Kytrain1 @ beta1).T)\n",
    "test_kcc1 = compute_correlation((Kxtest1 @ alpha1).T, (Kytest1 @ beta1).T)\n",
    "\n",
    "# perform quadratic KCCA\n",
    "kcc2, alpha2, beta2 = kcca_gep(Kxtrain2,Kytrain2,0.02,0.02,1)\n",
    "train_kcc2 = compute_correlation((Kxtrain2 @ alpha2).T, (Kytrain2 @ beta2).T)\n",
    "test_kcc2 = compute_correlation((Kxtest2 @ alpha2).T, (Kytest2 @ beta2).T)\n",
    "    \n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "plt.subplot(221)\n",
    "plt.plot(Kxtrain1 @ alpha1, Kytrain1 @ beta1, 'bs')\n",
    "plt.ylabel(r'$\\mathbf{K}_b^{train} \\beta$')\n",
    "plt.xlabel(r'$\\mathbf{K}_a^{train} \\alpha$')\n",
    "plt.title(r'Linear KCCA: $\\rho_{train}$ = %1.3f' % train_kcc1)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot((Kxtest1 @ alpha1), (Kytest1 @ beta1), 'bs')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.ylabel(r'$\\mathbf{K}_b^{test} \\beta$')\n",
    "plt.xlabel(r'$\\mathbf{K}_a^{test} \\alpha$')\n",
    "plt.title(r'Linear KCCA: $\\rho_{test}$ = %1.3f' % test_kcc1)\n",
    "    \n",
    "plt.subplot(223)\n",
    "plt.plot(Kxtrain2 @ alpha2, Kytrain2 @ beta2, 'bs')\n",
    "plt.ylabel(r'$\\mathbf{K}_b^{train} \\beta$')\n",
    "plt.xlabel(r'$\\mathbf{K}_a^{train} \\alpha$')\n",
    "plt.title(r'Quadratic KCCA: $\\rho_{train}$ = %1.3f' % train_kcc2)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.plot((Kxtest2 @ alpha2), (Kytest2 @ beta2), 'bs')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.ylabel(r'$\\mathbf{K}_b^{test} \\beta$')\n",
    "plt.xlabel(r'$\\mathbf{K}_a^{test} \\alpha$')\n",
    "plt.title(r'Quadratic KCCA: $\\rho_{test}$ = %1.3f' % test_kcc2)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer here to the following questions.\n",
    "1. Linear KCCA performs poorly on this dataset. How would CCA perform?\n",
    "2. Can we interpret the dual coefficient vectors $\\boldsymbol \\alpha$ and $\\boldsymbol \\beta$ in similar manner as $\\mathbf{w}_a$ and $\\mathbf{w}_b$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
